#First Spider codefrom scrapy.spider import BaseSpiderfrom scrapy.selector import HtmlXPathSelectorfrom tutorial.items import TutorialItemclass DmozSpider(BaseSpider):	name="dmoz"	allowed_domains = ["dmoz.org"]	start_urls = [		#"http://tech.sina.com.cn/t/2013-07-10/01268523473.shtml"		#"http://www.dmoz.org/Computers/Programming/Languages/Python/Books/",		#"http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/"		"http://bbs.tianya.cn/list-79-1.shtml"	]	def parsefile(self,response):		filename = response.url.split("/")[-2]		open(filename,'wb').write(response.body)	def parsejson(self,response):		hxs = HtmlXPathSelector(response)		sites = hxs.select('//fieldset/ul/li')		items = []		for site in sites:			item = TutorialItem()			item['title'] = site.select('a/text()').extract()			item['link'] = site.select('a/@href').extract()			item['desc'] = site.select('text()').extract()			items.append(item)		return items			def parse(self,response):		hxs = HtmlXPathSelector(response)		items = []		sites = hxs.select('//*[@id="main"]/div/table/tbody')		for site in sites:			siteitems = site.select('tr')			for siteitem in siteitems:				item = TutorialItem()				item['title'] = siteitem.select('td/a/text()').extract()				item['link'] = siteitem.select('td/a/@href').extract()				item['author'] = siteitem.select('td[2]/a/text()').extract()				item['click'] = siteitem.select('td[3]/text()').extract()				item['reply'] = siteitem.select('td[4]/text()').extract()				item['ftime'] = siteitem.select('td[5]/@title').extract()				items.append(item)		return items			